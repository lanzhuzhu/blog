{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"b48c4f7d61a5928be717d4bd654481ff1eab36ee","modified":1491654291000},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1491654657000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1491654657000},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1491654657000},{"_id":"themes/next/.gitignore","hash":"5f09fca02e030b7676c1d312cd88ce8fbccf381c","modified":1491654657000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1491654657000},{"_id":"themes/next/.javascript_ignore","hash":"f9ea3c5395f8feb225a24e2c32baa79afda30c16","modified":1491654657000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1491654657000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1491654657000},{"_id":"themes/next/.travis.yml","hash":"c42d9608c8c7fe90de7b1581a8dc3886e90c179e","modified":1491654657000},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1491654657000},{"_id":"themes/next/README.en.md","hash":"4ece25ee5f64447cd522e54cb0fffd9a375f0bd4","modified":1491654657000},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1491654657000},{"_id":"themes/next/_config.yml","hash":"8a82c1490df59aeed1809a37ca4c6ab28e31c3e0","modified":1491654657000},{"_id":"themes/next/bower.json","hash":"5abc236d9cc2512f5457ed57c1fba76669eb7399","modified":1491654657000},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1491654657000},{"_id":"themes/next/package.json","hash":"7e87b2621104b39a30488654c2a8a0c6a563574b","modified":1491654657000},{"_id":"source/_posts/Sentence Similarity Learning by Lexical Decomposition and Composition.md","hash":"e2f88f64f2776bd1c2f3a033ad88995b7af9f4d3","modified":1491651303000},{"_id":"source/_posts/hello-world.md","hash":"8a02477044e2b77f1b262da2c48c01429e4a32e4","modified":1491653396000},{"_id":"source/_posts/post.md","hash":"b993a12c5d3f52d9680f5170f2135ccc2368f035","modified":1491654759000},{"_id":"source/_posts/skim.md","hash":"14596d71d2396657ccbd6c786c9a927435b832e7","modified":1491654811000},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1491654657000},{"_id":"themes/next/.git/config","hash":"bf7d1df65cf34d0f25a7184a58c37a09f72e4be7","modified":1491654657000},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1491654125000},{"_id":"themes/next/.git/index","hash":"f2875cc55443792539416bf3f19be94ebd8993da","modified":1491654657000},{"_id":"themes/next/.git/packed-refs","hash":"537484e655ffd502d04aa6013807b50aaa65cfb3","modified":1491654657000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1491654657000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"fdd63b77472612337309eb93ec415a059b90756b","modified":1491654657000},{"_id":"themes/next/languages/de.yml","hash":"306db8c865630f32c6b6260ade9d3209fbec8011","modified":1491654657000},{"_id":"themes/next/languages/default.yml","hash":"4cc6aeb1ac09a58330e494c8771773758ab354af","modified":1491654657000},{"_id":"themes/next/languages/fr-FR.yml","hash":"24180322c83587a153cea110e74e96eacc3355ad","modified":1491654657000},{"_id":"themes/next/languages/en.yml","hash":"e7def07a709ef55684490b700a06998c67f35f39","modified":1491654657000},{"_id":"themes/next/languages/id.yml","hash":"2835ea80dadf093fcf47edd957680973f1fb6b85","modified":1491654657000},{"_id":"themes/next/languages/ja.yml","hash":"1c3a05ab80a6f8be63268b66da6f19da7aa2c638","modified":1491654657000},{"_id":"themes/next/languages/ko.yml","hash":"be150543379150f78329815af427bf152c0e9431","modified":1491654657000},{"_id":"themes/next/languages/pt-BR.yml","hash":"958e49571818a34fdf4af3232a07a024050f8f4e","modified":1491654657000},{"_id":"themes/next/languages/pt.yml","hash":"36c8f60dacbe5d27d84d0e0d6974d7679f928da0","modified":1491654657000},{"_id":"themes/next/languages/ru.yml","hash":"7462c3017dae88e5f80ff308db0b95baf960c83f","modified":1491654657000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"3c0c7dfd0256457ee24df9e9879226c58cb084b5","modified":1491654657000},{"_id":"themes/next/languages/zh-hk.yml","hash":"1c917997413bf566cb79e0975789f3c9c9128ccd","modified":1491654657000},{"_id":"themes/next/languages/zh-tw.yml","hash":"0b2c18aa76570364003c8d1cd429fa158ae89022","modified":1491654657000},{"_id":"themes/next/layout/_layout.swig","hash":"909d68b164227fe7601d82e2303bf574eb754172","modified":1491654657000},{"_id":"themes/next/layout/archive.swig","hash":"b5b59d70fc1563f482fa07afd435752774ad5981","modified":1491654657000},{"_id":"themes/next/layout/category.swig","hash":"6422d196ceaff4220d54b8af770e7e957f3364ad","modified":1491654657000},{"_id":"themes/next/layout/index.swig","hash":"427d0b95b854e311ae363088ab39a393bf8fdc8b","modified":1491654657000},{"_id":"themes/next/layout/page.swig","hash":"3727fab9dadb967e9c2204edca787dc72264674a","modified":1491654657000},{"_id":"themes/next/layout/post.swig","hash":"e2e512142961ddfe77eba29eaa88f4a2ee43ae18","modified":1491654657000},{"_id":"themes/next/layout/schedule.swig","hash":"234dc8c3b9e276e7811c69011efd5d560519ef19","modified":1491654657000},{"_id":"themes/next/layout/tag.swig","hash":"07cf49c49c39a14dfbe9ce8e7d7eea3d4d0a4911","modified":1491654657000},{"_id":"themes/next/scripts/merge-configs.js","hash":"13c8b3a2d9fce06c2488820d9248d190c8100e0a","modified":1491654657000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1491654657000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1491654657000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1491654657000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1491654657000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1491654657000},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1491654125000},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1491654125000},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1491654125000},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1491654125000},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1491654125000},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1491654125000},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1491654125000},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1491654125000},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1491654125000},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1491654125000},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1491654125000},{"_id":"themes/next/.git/logs/HEAD","hash":"d916c8704992149ffdc305485d1befc77ec79ebf","modified":1491654657000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1491654657000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1491654657000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"5864f5567ba5efeabcf6ea355013c0b603ee07f2","modified":1491654657000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"b16fcbf0efd20c018d7545257a8533c497ea7647","modified":1491654657000},{"_id":"themes/next/layout/_macro/post.swig","hash":"640b431eccbbd27f10c6781f33db5ea9a6e064de","modified":1491654657000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1491654657000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"911b99ba0445b2c07373128d87a4ef2eb7de341a","modified":1491654657000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1491654657000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"1c7d3c975e499b9aa3119d6724b030b7b00fc87e","modified":1491654657000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"7172c6053118b7c291a56a7860128a652ae66b83","modified":1491654657000},{"_id":"themes/next/layout/_partials/head.swig","hash":"a0eafe24d1dae30c790ae35612154b3ffbbd5cce","modified":1491654657000},{"_id":"themes/next/layout/_partials/header.swig","hash":"a1ffbb691dfad3eaf2832a11766e58a179003b8b","modified":1491654657000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1491654657000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1491654657000},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1491654657000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1491654657000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1491654657000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9de352a32865869e7ed6863db271c46db5853e5a","modified":1491654657000},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1491654657000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1491654657000},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1491654657000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1491654657000},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1491654657000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1491654657000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1491654657000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1491654657000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1491654657000},{"_id":"themes/next/scripts/tags/note.js","hash":"6752925eedbdb939d8ec4d11bdfb75199f18dd70","modified":1491654657000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1491654657000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1491654657000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1491654657000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1491654657000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1491654657000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1491654657000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1491654657000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1491654657000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1491654657000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1491654657000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1491654657000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1491654657000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1491654657000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1491654657000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1491654657000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1491654657000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1491654657000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1491654657000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1491654657000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1491654657000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1491654657000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1491654657000},{"_id":"themes/next/.git/refs/heads/master","hash":"929f0a39f61901b4b7ead241fc06f16e4584928a","modified":1491654657000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1491654657000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1491654657000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"2d1075f4cabcb3956b7b84a8e210f5a66f0a5562","modified":1491654657000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1491654657000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1491654657000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1491654657000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1491654657000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1491654657000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1491654657000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1491654657000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1491654657000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1491654657000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1491654657000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1491654657000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1491654657000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1491654657000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1491654657000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"3358d11b9a26185a2d36c96049e4340e701646e4","modified":1491654657000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1491654657000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1491654657000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"a652f202bd5b30c648c228ab8f0e997eb4928e44","modified":1491654657000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1491654657000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1491654657000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1491654657000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1491654657000},{"_id":"themes/next/layout/_third-party/comments/gentie.swig","hash":"03592d1d731592103a41ebb87437fe4b0a4c78ca","modified":1491654657000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1491654657000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"abb92620197a16ed2c0775edf18a0f044a82256e","modified":1491654657000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"7240f2e5ec7115f8abbbc4c9ef73d4bed180fdc7","modified":1491654657000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"af9dd8a4aed7d06cf47b363eebff48850888566c","modified":1491654657000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1491654657000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"f4dbd4c896e6510ded8ebe05394c28f8a86e71bf","modified":1491654657000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1491654657000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1491654657000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1491654657000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1491654657000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1491654657000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1491654657000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"06f432f328a5b8a9ef0dbd5301b002aba600b4ce","modified":1491654657000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"28a7f84242ca816a6452a0a79669ca963d824607","modified":1491654657000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1491654657000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1491654657000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"aab7be0a6e2724b3faa9338db93c19556c559625","modified":1491654657000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1491654657000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1491654657000},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1491654657000},{"_id":"themes/next/source/js/src/post-details.js","hash":"af7a417dd1cb02465a7b98211653e7c6192e6d55","modified":1491654657000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1491654657000},{"_id":"themes/next/source/js/src/utils.js","hash":"e13c9ccf70d593bdf3b8cc1d768f595abd610e6e","modified":1491654657000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1491654657000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1491654657000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1491654657000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1491654657000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1491654657000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1491654657000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1491654657000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1491654657000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1491654657000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1491654657000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1491654657000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1491654657000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1491654657000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"5b38ae00297ffc07f433c632c3dbf7bde4cdf39a","modified":1491654657000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1491654657000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1491654657000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1491654657000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1491654657000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1491654657000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1491654657000},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"d916c8704992149ffdc305485d1befc77ec79ebf","modified":1491654657000},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1491654657000},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1491654657000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"59ad08bcc6fe9793594869ac2b4c525021453e78","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"ef089a407c90e58eca10c49bc47ec978f96e03ba","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1491654657000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1491654657000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"7804e31c44717c9a9ddf0f8482b9b9c1a0f74538","modified":1491654657000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1491654657000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1491654657000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1491654657000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e3e23751d4ad24e8714b425d768cf68e37de7ded","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1491654657000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1491654657000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1491654657000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1491654657000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1491654657000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1491654657000},{"_id":"themes/next/.git/objects/pack/pack-d2e689db66e027ccdba615b3076c463c3c28705d.idx","hash":"0c2e04ed21f69df5f0b65a1a241d861428690a3e","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1491654657000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1491654657000},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"d916c8704992149ffdc305485d1befc77ec79ebf","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"740d37f428b8f4574a76fc95cc25e50e0565f45e","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"beccb53dcd658136fb91a0c5678dea8f37d6e0b6","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"8fae54591877a73dff0b29b2be2e8935e3c63575","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"c196401747019d389da09b7a0fe7f27e3a0ec01f","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"b9a2e76f019a5941191f1263b54aef7b69c48789","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"8c0276883398651336853d5ec0e9da267a00dd86","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"5f6ea57aabfa30a437059bf8352f1ad829dbd4ff","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a2ec22ef4a6817bbb2abe8660fcd99fe4ca0cc5e","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"74d0ba86f698165d13402670382a822c8736a556","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"dd310c2d999185e881db007360176ee2f811df10","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/third-party/gentie.styl","hash":"586a3ec0f1015e7207cd6a2474362e068c341744","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"173490e21bece35a34858e8e534cf86e34561350","modified":1491654657000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"42348219db93a85d2ee23cb06cebd4d8ab121726","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1491654657000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1491654657000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1491654657000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1491654657000},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1491654657000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1491654657000},{"_id":"themes/next/.git/objects/pack/pack-d2e689db66e027ccdba615b3076c463c3c28705d.pack","hash":"63d22546f15f2e1ce9315fc721105f47853da8c3","modified":1491654657000},{"_id":"public/2017/03/07/Sentence Similarity Learning by Lexical Decomposition and Composition/index.html","hash":"27bea9b8fc86f48b3f187921cdbeda627b3681c1","modified":1491657001334},{"_id":"public/2017/04/08/hello-world/index.html","hash":"44773697a4e303fcdfaf65c1574600b30b752c41","modified":1491657001334},{"_id":"public/2017/04/08/post/index.html","hash":"cf5c1fa54b57367983bac3eb17f032acf4dccc8c","modified":1491657001334},{"_id":"public/archives/index.html","hash":"8d04a060b0f52c4e51d1343c6efa85f24d73e846","modified":1491657001334},{"_id":"public/archives/2017/index.html","hash":"9270f5d03e9c4f46fecb466f6c822256a2c253cf","modified":1491657001334},{"_id":"public/archives/2017/03/index.html","hash":"b982e2235fc1b2b93766999d34abffc88ce68065","modified":1491657001334},{"_id":"public/archives/2017/04/index.html","hash":"cd740db3b1f2bee7ea7f9c0f3111fdab5a5bf5a8","modified":1491657001334},{"_id":"public/tags/自然语言处理/index.html","hash":"0f99db46cfdbd18a0291c1f514ae6c41b63c4120","modified":1491657001334},{"_id":"public/2017/03/12/skim/index.html","hash":"9c790f78a82304b385da096796e4404daa2878b2","modified":1491657001334},{"_id":"public/index.html","hash":"c4319841574025c572a0cc480a2f586640750e71","modified":1491657001334},{"_id":"public/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1491657001350},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1491657001350},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1491657001350},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1491657001350},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1491657001350},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1491657001350},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1491657001350},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1491657001350},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1491657001350},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1491657001350},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1491657001350},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1491657001350},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1491657001350},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1491657001350},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1491657001350},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1491657001350},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1491657001350},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1491657001350},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1491657001350},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1491657001351},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1491657001351},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1491657001351},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1491657001351},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1491657001351},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1491657001901},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1491657001910},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1491657001921},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1491657001921},{"_id":"public/js/src/bootstrap.js","hash":"aab7be0a6e2724b3faa9338db93c19556c559625","modified":1491657001922},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1491657001922},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1491657001922},{"_id":"public/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1491657001922},{"_id":"public/js/src/post-details.js","hash":"af7a417dd1cb02465a7b98211653e7c6192e6d55","modified":1491657001922},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1491657001922},{"_id":"public/js/src/utils.js","hash":"e13c9ccf70d593bdf3b8cc1d768f595abd610e6e","modified":1491657001922},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1491657001922},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1491657001922},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1491657001922},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1491657001922},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1491657001922},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1491657001922},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1491657001922},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1491657001922},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1491657001922},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1491657001922},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1491657001922},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1491657001922},{"_id":"public/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1491657001922},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1491657001922},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1491657001922},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1491657001922},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1491657001922},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1491657001922},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1491657001923},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1491657001923},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1491657001923},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1491657001923},{"_id":"public/css/main.css","hash":"eeb35019d995e2d1630c87a448f186fdf7947a53","modified":1491657001923},{"_id":"public/lib/three/three-waves.min.js","hash":"5b38ae00297ffc07f433c632c3dbf7bde4cdf39a","modified":1491657001923},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1491657001923},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1491657001923},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1491657001923},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1491657001923},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1491657001923},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1491657001923},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1491657001923},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1491657001923},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1491657001924},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1491657001924},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1491657001924},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1491657001924},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1491657001924},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1491657001924},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1491657001935}],"Category":[],"Data":[],"Page":[],"Post":[{"_content":"## Sentence Similarity Learning by Lexical Decomposition and Composition\n- ZhiguoWang and Haitao Mi and Abraham Ittycheriah\n- IBM T.J. Watson Research Center\n- Yorktown Heights, NY, USA\n- 2016.2.23\n\n### 摘要\n传统句子相似性方法关注输入两个句子的相似部分，易忽略不相似部分。本文提出的模型，通过分解和组合词汇的语义考虑了句子的相似性和不相似性。在answer\nsentence selection实现了最佳成绩。\n\n### introduction\n1. 【**背景意义**】计算句子相似性的重要性，举例，在NLP/IR 领域，在PI（paraphrase identification），QA，IR 任务\n2. 【**问题挑战**】然而，相似性学习有如下问题：\n> - 语义相似词汇不同\n> - 语义相似性在不同粒度级别上的测量（词、短语、句法）\n> - 两个句子的不同部分是重要线索，如何利用？\n\n3. 【**已有研究及不足**】为了解决上述问题的已有研究，分别对三个问题回答：1）已经提出的基于知识和基于语料的词汇相似性测量方法 2）利用从n-grams，连续短语，不连续短语中抽取特征；3）利用句子间不同作PI任务，需要人工标注数据。【少研究，已有的不足。】\n4. 【**本文工作**】 介绍系统几个步骤，解决什么问题\n5. 【**文章组织**】\n\n### 句子相似性模型\n\n难点：句子的分解，计算每个句子的每个词和另一个句子对应的相关度公式5。然后根据**匹配的相关度**得到两个矩阵表示，自己这个句子的每个词，在另一个句子上最相关的语义表达，公式6，三种方法。公式7-9也记录了三种利用公式6，将原来输入的句子分解成相似和不相似的两部分，有严格匹配，线性和正交三种分解法。\n\n\n\n","source":"_posts/Sentence Similarity Learning by Lexical Decomposition and Composition.md","raw":"## Sentence Similarity Learning by Lexical Decomposition and Composition\n- ZhiguoWang and Haitao Mi and Abraham Ittycheriah\n- IBM T.J. Watson Research Center\n- Yorktown Heights, NY, USA\n- 2016.2.23\n\n### 摘要\n传统句子相似性方法关注输入两个句子的相似部分，易忽略不相似部分。本文提出的模型，通过分解和组合词汇的语义考虑了句子的相似性和不相似性。在answer\nsentence selection实现了最佳成绩。\n\n### introduction\n1. 【**背景意义**】计算句子相似性的重要性，举例，在NLP/IR 领域，在PI（paraphrase identification），QA，IR 任务\n2. 【**问题挑战**】然而，相似性学习有如下问题：\n> - 语义相似词汇不同\n> - 语义相似性在不同粒度级别上的测量（词、短语、句法）\n> - 两个句子的不同部分是重要线索，如何利用？\n\n3. 【**已有研究及不足**】为了解决上述问题的已有研究，分别对三个问题回答：1）已经提出的基于知识和基于语料的词汇相似性测量方法 2）利用从n-grams，连续短语，不连续短语中抽取特征；3）利用句子间不同作PI任务，需要人工标注数据。【少研究，已有的不足。】\n4. 【**本文工作**】 介绍系统几个步骤，解决什么问题\n5. 【**文章组织**】\n\n### 句子相似性模型\n\n难点：句子的分解，计算每个句子的每个词和另一个句子对应的相关度公式5。然后根据**匹配的相关度**得到两个矩阵表示，自己这个句子的每个词，在另一个句子上最相关的语义表达，公式6，三种方法。公式7-9也记录了三种利用公式6，将原来输入的句子分解成相似和不相似的两部分，有严格匹配，线性和正交三种分解法。\n\n\n\n","slug":"Sentence Similarity Learning by Lexical Decomposition and Composition","published":1,"date":"2017-03-07T07:12:11.000Z","updated":"2017-04-08T11:35:03.000Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cj19a2bve0000dcfy2syufxqo","content":"<h2 id=\"Sentence-Similarity-Learning-by-Lexical-Decomposition-and-Composition\"><a href=\"#Sentence-Similarity-Learning-by-Lexical-Decomposition-and-Composition\" class=\"headerlink\" title=\"Sentence Similarity Learning by Lexical Decomposition and Composition\"></a>Sentence Similarity Learning by Lexical Decomposition and Composition</h2><ul>\n<li>ZhiguoWang and Haitao Mi and Abraham Ittycheriah</li>\n<li>IBM T.J. Watson Research Center</li>\n<li>Yorktown Heights, NY, USA</li>\n<li>2016.2.23</li>\n</ul>\n<h3 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h3><p>传统句子相似性方法关注输入两个句子的相似部分，易忽略不相似部分。本文提出的模型，通过分解和组合词汇的语义考虑了句子的相似性和不相似性。在answer<br>sentence selection实现了最佳成绩。</p>\n<h3 id=\"introduction\"><a href=\"#introduction\" class=\"headerlink\" title=\"introduction\"></a>introduction</h3><ol>\n<li>【<strong>背景意义</strong>】计算句子相似性的重要性，举例，在NLP/IR 领域，在PI（paraphrase identification），QA，IR 任务</li>\n<li><p>【<strong>问题挑战</strong>】然而，相似性学习有如下问题：</p>\n<blockquote>\n<ul>\n<li>语义相似词汇不同</li>\n<li>语义相似性在不同粒度级别上的测量（词、短语、句法）</li>\n<li>两个句子的不同部分是重要线索，如何利用？</li>\n</ul>\n</blockquote>\n</li>\n<li><p>【<strong>已有研究及不足</strong>】为了解决上述问题的已有研究，分别对三个问题回答：1）已经提出的基于知识和基于语料的词汇相似性测量方法 2）利用从n-grams，连续短语，不连续短语中抽取特征；3）利用句子间不同作PI任务，需要人工标注数据。【少研究，已有的不足。】</p>\n</li>\n<li>【<strong>本文工作</strong>】 介绍系统几个步骤，解决什么问题</li>\n<li>【<strong>文章组织</strong>】</li>\n</ol>\n<h3 id=\"句子相似性模型\"><a href=\"#句子相似性模型\" class=\"headerlink\" title=\"句子相似性模型\"></a>句子相似性模型</h3><p>难点：句子的分解，计算每个句子的每个词和另一个句子对应的相关度公式5。然后根据<strong>匹配的相关度</strong>得到两个矩阵表示，自己这个句子的每个词，在另一个句子上最相关的语义表达，公式6，三种方法。公式7-9也记录了三种利用公式6，将原来输入的句子分解成相似和不相似的两部分，有严格匹配，线性和正交三种分解法。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Sentence-Similarity-Learning-by-Lexical-Decomposition-and-Composition\"><a href=\"#Sentence-Similarity-Learning-by-Lexical-Decomposition-and-Composition\" class=\"headerlink\" title=\"Sentence Similarity Learning by Lexical Decomposition and Composition\"></a>Sentence Similarity Learning by Lexical Decomposition and Composition</h2><ul>\n<li>ZhiguoWang and Haitao Mi and Abraham Ittycheriah</li>\n<li>IBM T.J. Watson Research Center</li>\n<li>Yorktown Heights, NY, USA</li>\n<li>2016.2.23</li>\n</ul>\n<h3 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h3><p>传统句子相似性方法关注输入两个句子的相似部分，易忽略不相似部分。本文提出的模型，通过分解和组合词汇的语义考虑了句子的相似性和不相似性。在answer<br>sentence selection实现了最佳成绩。</p>\n<h3 id=\"introduction\"><a href=\"#introduction\" class=\"headerlink\" title=\"introduction\"></a>introduction</h3><ol>\n<li>【<strong>背景意义</strong>】计算句子相似性的重要性，举例，在NLP/IR 领域，在PI（paraphrase identification），QA，IR 任务</li>\n<li><p>【<strong>问题挑战</strong>】然而，相似性学习有如下问题：</p>\n<blockquote>\n<ul>\n<li>语义相似词汇不同</li>\n<li>语义相似性在不同粒度级别上的测量（词、短语、句法）</li>\n<li>两个句子的不同部分是重要线索，如何利用？</li>\n</ul>\n</blockquote>\n</li>\n<li><p>【<strong>已有研究及不足</strong>】为了解决上述问题的已有研究，分别对三个问题回答：1）已经提出的基于知识和基于语料的词汇相似性测量方法 2）利用从n-grams，连续短语，不连续短语中抽取特征；3）利用句子间不同作PI任务，需要人工标注数据。【少研究，已有的不足。】</p>\n</li>\n<li>【<strong>本文工作</strong>】 介绍系统几个步骤，解决什么问题</li>\n<li>【<strong>文章组织</strong>】</li>\n</ol>\n<h3 id=\"句子相似性模型\"><a href=\"#句子相似性模型\" class=\"headerlink\" title=\"句子相似性模型\"></a>句子相似性模型</h3><p>难点：句子的分解，计算每个句子的每个词和另一个句子对应的相关度公式5。然后根据<strong>匹配的相关度</strong>得到两个矩阵表示，自己这个句子的每个词，在另一个句子上最相关的语义表达，公式6，三种方法。公式7-9也记录了三种利用公式6，将原来输入的句子分解成相似和不相似的两部分，有严格匹配，线性和正交三种分解法。</p>\n"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"date":"2017-04-08T12:09:56.000Z","updated":"2017-04-08T12:09:56.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj19a2bvj0001dcfydw1v4cdm","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"external\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"external\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"external\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"external\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"external\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"external\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"external\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"external\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\">Deployment</a></p>\n"},{"title":"[object Object]","date":"2017-04-08T12:09:56.000Z","_content":"\n","source":"_posts/post.md","raw":"---\ntitle: {{ title }}\ndate: {{ date }}\ntags:\n---\n\n","slug":"post","published":1,"updated":"2017-04-08T12:32:39.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj19a2bvr0002dcfy7devgqkq","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"[object Object]","date":"2017-03-12T14:28:18.000Z","_content":"\n## 1. Sequence to Sequence Learning with Neural Networks\n\nDNN 不能用来map sequences to sequences.本文提出一个 一般的end-to-end方法可以去学习变长结构的序列。使用多层LSTM去匹配输入到一个固定维度向量，然后另一个deep LSTM从该中间向量解码出目标句子。\n\n![s2sModel](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi2t7sejj30m307kq36.jpg)\n\n![s2sEq](https://ww1.sinaimg.cn/large/006tNbRwgy1fefi2z84uhj30j9020743.jpg)\n\nLSTM的目标是估计出左边的最大概率。首先计算出固定维度表达V，表示 the input sequence $ (x_1,…,x_T)$ ,即LSTM的最后一个隐藏状态。每个右边子公式的分布是用词典里所有词的softmax表示。每个句子使用EOS结束符号，使得模型能够定义所有可能长度的句子分布。最大化log概率，训练目标：其中S是很多句子对的训练集。![s2sloss](/Users/vera\\百度云同步盘\\PHD\\reread paper\\pic\\s2sloss.PNG)\n\n**result :** \n\n![S2Sresult2](/Users/vera/百度云同步盘\\PHD\\reread paper\\pic\\S2Sresult2.PNG)\n\n**conclusion:** 编码最大数量的短期依赖，使学习问题变得简单。反转源语言句子使结果提高很多，也是因为此。\n\n---\n\n## 2. aNMM: Ranking Short Answer Texts with Attention-Based  Neural Matching Model\n\n#### 文章来源：\n\n**CIKM’16** , October 24-28, 2016, Indianapolis, IN, USA\n\n==Liu Yan==g 1  Qingyao Ai 1 Jiafeng Guo 2 W. Bruce Croft 1\n\n1. Center for Intelligent Information Retrieval, University of Massachusetts Amherst, MA, USA\n\n\n2.  CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\n\n{lyang, aiqy, croft}@cs.umass.edu, guojiafeng@ict.ac.cn\n\n题外话：本文的实现是基于java的，难怪最后模型是softmax那样的训练方法。https://github.com/yangliuy/aNMM-CIKM16 开源程序。\n\n#### 要解决的问题和已有方法：\n\n基于attention的神经匹配模型去排序短文本；数据集TREC QA。\n\n之前模型，结合额外特征，如word overlap& BM25;没有这些特征，模型表现差于语言特征工程。\n\n本文解决问题：RQ1：不结合额外特征，DL模型能实现或者超过使用特征工程的方法么？\n\nRQ2：结合额外特征，本文模型能优于当前QA的最好模型么？\n\n之前不足：\n\n1. 之前模型的不止为QA匹配：CNN使用position-valued权重，LSTM问答对序列，没有直接交互不能获得有效匹配信号。\n2. 缺少建模问题的关注点。\n\n#### 本文方法和创新点\n\n1. 采用value-shared weighting scheme 而不是 position-shared weighting来结合匹配信号，\n\n2. 并且包含问题词重要性学习，通过问题attention 网络。\n\n3. 拓展的实验评估和不错的结果。\n\n   ![model2017-03-30 下午4.46.32](https://ww2.sinaimg.cn/large/006tNbRwgy1fefi32hfmgj31kw0w4wxi.jpg)\n\n   从输入层到隐藏层的计算：valued-shared weighting，如何体现的？是否有意义？\n\n   ![eq2017-03-30 下午4.48.23](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi33cmi4j30f403wmxa.jpg)\n\n   从隐藏层到输出层（还有一个softmax 门学习question权重）：question attention network,通过这个attention网络的输出 来 给定“  问答对匹配信号”的 权重  即这里的v，这里使用了nomalize，归一化。\n\n   ![屏幕快照 2017-03-30 下午5.00.52](https://ww3.sinaimg.cn/large/006tNbRwgy1fefi34mgmtj30to050t9b.jpg)\n\n   aNMM-2增加了$r_t$  T为隐藏向量节点数，图中多个黄色点。w为多个权重向量，二维矩阵。两个隐藏层，相比于上个公式，1）增加线性结合的结点输出，2）额外激活函数$\\tau$ 。![屏幕快照 2017-03-30 下午5.57.32](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi39bmd3j30p80583yz.jpg)\n\n   推导反向传播的公式，损失函数含义？\n\n   ​\n#### 效果和评价\n\n   可以看出ANMM1模型效果其实更好，而且加入了attention提高不多。和我biLSTM/cnn差不多。\n\n   它的实验使用的==word embeding 是700维==(300-700)维较好，在验证集上测试，可以试试。\n\n   ![屏幕快照 2017-03-30 下午8.49.11](https://ww3.sinaimg.cn/large/006tNbRwgy1fefi3bedcvj30ya110wol.jpg)\n\n评价：attention 比较简单，就是加了一个可训练的权重v。至于value-shared weighting，说不上效果如何，有多大提高，本文使用number of bins=600，比较细粒度的对权重进行分类学习。问答对的匹配的信号范围[-1，1]，由cosine计算得到。如下图，说不上这个value-shared weighting有什么规律？！![屏幕快照 2017-03-30 下午9.09.59](https://ww1.sinaimg.cn/large/006tNbRwgy1fefi3cvnrzj30ya110wol.jpg)\n\n## 3. Bilateral Multi-Perspective Matching for Natural Language Sentences\n\n#### 文章来源\n\n- Zhiguo Wang(王志国), Wael Hamza and Radu Florian. 2017.2.28   In eprint arXiv:1702.03814.\n- IBM T.J. Watson Research Center 1101 Kitchawan Rd, Yorktown Heights, NY 10598 {zhigwang,whamza,raduf}@us.ibm.com\n\n#### 要解决的问题和已有方法\n\n==自然语言句子匹配==是很多任务的基础，之前的工作要么是单向的，要么只应用了一种粗糙（granular,eg,word-by-word ,sentence-by-sentence）的匹配，本文在三个任务上paraphrase identiﬁcation（PI）, natural language inference(自然语言推理) and answer sentence selection（AS）都取得了state-of-art成绩。\n\n==Introduction:== NLSM(Natural language sentence matching)任务是……，在PI、推论、AS等任务中是基础。\n\nNLSM任务中已提出两种DL框架，1.“Siamese” architecture，独立处理两个输入的句子，匹配决定基于两个句子向量。 优点是，共享参数使模型更小、易训练。缺点，编码过程无交互，损失信息。2. “Matching aggregation”框架，两个句子更小粒度匹配，然后集合匹配结果。然而之前的“MG”方法有限制：只探索了word-by-word 匹配，忽略了其他粒度匹配；匹配是单向的，忽略相反方向。\n\n#### 本文的方法和创新点\n\n给定两个句子P\\Q，用biLSTM对他们分别编码，从两个方向P->Q,Q->P对其匹配。\n\n在匹配过程中，从多视野的角度，一个句子的每一步都与另一个句子的所有time-steps对应匹配。然后，另一个BiLSTM被用来集合所有匹配结果到一个固定长度的向量。再连上一个全连接层得到匹配的结果。\n\n##### 模型总览：\n\n本文提出了BiMPM(bilateral multi-perspective matching)，从下图可以看出分为5个层，\n\n![屏幕快照 2017-03-31 下午9.19.01](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3dqov0j31kw0fhdqa.jpg)\n\n其中1：第一层说词表达层由两部分组成：word embeddings and character-composed embeddings（后者是把一个词的每个字符放到LST==M里？==pre-trained with GloVe [ Pennington et al., 2014 ] or word2vec [ Mikolov et al., ] .）\n\n2. Context Representation Layer就是双向LSTM处理。\n\n3. 匹配层。本文的关键创新点，多视角的匹配操作，具体见图2.\n\n4. 第四层，结合层，就是用BiLSTM处理匹配之后的3的输出向量，这里采用最后一个时刻的向量进行连接，图中的四个绿色向量。\n\n5. 预测层，两个前向神经网络处理固定长度的匹配向量（4）的输出，最后加一个softmax层分类输出。\n\n##### 下面对于3进行详细解释：\n\n   ​\n\n   ![屏幕快照 2017-03-31 下午10.23.42](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3efja4j31kw0fhdqa.jpg)\n\n   A).  图中四种不同视角的计算匹配方法，都是用了余弦匹配$f_m$来计算，返回向量$m = f_m(v_1,v_2;W)$ ，计算两个d维向量的匹配值，$W \\in R^ {l \\times d}$ ,可训练参数。\n\n  返回向量$m=[m_1,…，m_k,…，m_l]$ ,其中$m_k = \\cos (W_k \\cdot v_1,W_K \\cdot v_2)$ ，l表示视图的维度，这里的$W_k$是W的第k行。每个P的time-step生成一个向量，即连接了8个生成的视图向量，（正反向2*4种视角）。\n\n   B). 图中四种视角只显示了单向P->Q, P中每个time-step与Q所有time-steps计算出一个m向量，总共实现双向。1）使用 Q的最后一个隐藏向量进行计算。![屏幕快照 2017-04-01 上午9.18.03](https://ww1.sinaimg.cn/large/006tNbRwgy1fefi3f1yzmj311i0vkgr5.jpg)\n\n   2）max-pooling matching: 与q的每个time-step计算fm，再求最大值。![屏幕快照 2017-04-01 上午9.21.01](https://ww3.sinaimg.cn/large/006tNbRwgy1fefi3flfngj312s12ewjh.jpg)\n\n   3）attentive-matching:计算与每个q的相关度权值，再加权求和，与这个含权值的向量计算$f_m$![屏幕快照 2017-04-01 上午9.23.24](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3g6chzj30qi06i0u2.jpg)\n\n   ![屏幕快照 2017-04-01 上午9.35.42](https://ww2.sinaimg.cn/large/006tNbRwgy1fefi3i449dj30km08ejsl.jpg)\n\n![屏幕快照 2017-04-01 上午9.36.38](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3jim9aj30m403yaav.jpg)\n\n   4)max-attentive matching,如图所示，就是在3）计算出权重值$\\alpha_{i,j}$的基础上，选出最高的cos相似值作为attentive向量，然后计算得$m_i = f_m (h^p_i,max(\\alpha_{i,j}) )for j \\in 1…N$.\n\n##### 实验的设置\n\n问题1：多视图中的l不是固定的8么？怎么设置成20？或者$l\\in\\{1,5,10,15,20\\}$实验？\n\n#### 效果和评价\n\n实验结果：在三个任务上的实验结果证明了模型的有效性。早期的工作设计手工创建的特征，只能在某一个任务或数据集上work well,很难在其他任务上普遍很好。 DL的框架，第一种，Siamese architecture，忽视了较低level的交互特征是必不可少，后期，从很多层次的粒度来匹配句子的模型越来越多，本文就属于这种框架。如下也证明了该框架。\n\n![屏幕快照 2017-04-01 上午11.34.43](https://ww2.sinaimg.cn/large/006tNbRwgy1fefi3n3nr7j31fe0qmqeb.jpg)\n\n![屏幕快照 2017-04-01 上午11.36.42](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3of5kzj31fe0qmqeb.jpg)","source":"_posts/skim.md","raw":"---\ntitle: {{  Sequence to Sequence Learning with Neural Networks }}\ndate: {{ 2017/03/05 }}\ntags: 自然语言处理\n\n---\n\n## 1. Sequence to Sequence Learning with Neural Networks\n\nDNN 不能用来map sequences to sequences.本文提出一个 一般的end-to-end方法可以去学习变长结构的序列。使用多层LSTM去匹配输入到一个固定维度向量，然后另一个deep LSTM从该中间向量解码出目标句子。\n\n![s2sModel](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi2t7sejj30m307kq36.jpg)\n\n![s2sEq](https://ww1.sinaimg.cn/large/006tNbRwgy1fefi2z84uhj30j9020743.jpg)\n\nLSTM的目标是估计出左边的最大概率。首先计算出固定维度表达V，表示 the input sequence $ (x_1,…,x_T)$ ,即LSTM的最后一个隐藏状态。每个右边子公式的分布是用词典里所有词的softmax表示。每个句子使用EOS结束符号，使得模型能够定义所有可能长度的句子分布。最大化log概率，训练目标：其中S是很多句子对的训练集。![s2sloss](/Users/vera\\百度云同步盘\\PHD\\reread paper\\pic\\s2sloss.PNG)\n\n**result :** \n\n![S2Sresult2](/Users/vera/百度云同步盘\\PHD\\reread paper\\pic\\S2Sresult2.PNG)\n\n**conclusion:** 编码最大数量的短期依赖，使学习问题变得简单。反转源语言句子使结果提高很多，也是因为此。\n\n---\n\n## 2. aNMM: Ranking Short Answer Texts with Attention-Based  Neural Matching Model\n\n#### 文章来源：\n\n**CIKM’16** , October 24-28, 2016, Indianapolis, IN, USA\n\n==Liu Yan==g 1  Qingyao Ai 1 Jiafeng Guo 2 W. Bruce Croft 1\n\n1. Center for Intelligent Information Retrieval, University of Massachusetts Amherst, MA, USA\n\n\n2.  CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\n\n{lyang, aiqy, croft}@cs.umass.edu, guojiafeng@ict.ac.cn\n\n题外话：本文的实现是基于java的，难怪最后模型是softmax那样的训练方法。https://github.com/yangliuy/aNMM-CIKM16 开源程序。\n\n#### 要解决的问题和已有方法：\n\n基于attention的神经匹配模型去排序短文本；数据集TREC QA。\n\n之前模型，结合额外特征，如word overlap& BM25;没有这些特征，模型表现差于语言特征工程。\n\n本文解决问题：RQ1：不结合额外特征，DL模型能实现或者超过使用特征工程的方法么？\n\nRQ2：结合额外特征，本文模型能优于当前QA的最好模型么？\n\n之前不足：\n\n1. 之前模型的不止为QA匹配：CNN使用position-valued权重，LSTM问答对序列，没有直接交互不能获得有效匹配信号。\n2. 缺少建模问题的关注点。\n\n#### 本文方法和创新点\n\n1. 采用value-shared weighting scheme 而不是 position-shared weighting来结合匹配信号，\n\n2. 并且包含问题词重要性学习，通过问题attention 网络。\n\n3. 拓展的实验评估和不错的结果。\n\n   ![model2017-03-30 下午4.46.32](https://ww2.sinaimg.cn/large/006tNbRwgy1fefi32hfmgj31kw0w4wxi.jpg)\n\n   从输入层到隐藏层的计算：valued-shared weighting，如何体现的？是否有意义？\n\n   ![eq2017-03-30 下午4.48.23](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi33cmi4j30f403wmxa.jpg)\n\n   从隐藏层到输出层（还有一个softmax 门学习question权重）：question attention network,通过这个attention网络的输出 来 给定“  问答对匹配信号”的 权重  即这里的v，这里使用了nomalize，归一化。\n\n   ![屏幕快照 2017-03-30 下午5.00.52](https://ww3.sinaimg.cn/large/006tNbRwgy1fefi34mgmtj30to050t9b.jpg)\n\n   aNMM-2增加了$r_t$  T为隐藏向量节点数，图中多个黄色点。w为多个权重向量，二维矩阵。两个隐藏层，相比于上个公式，1）增加线性结合的结点输出，2）额外激活函数$\\tau$ 。![屏幕快照 2017-03-30 下午5.57.32](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi39bmd3j30p80583yz.jpg)\n\n   推导反向传播的公式，损失函数含义？\n\n   ​\n#### 效果和评价\n\n   可以看出ANMM1模型效果其实更好，而且加入了attention提高不多。和我biLSTM/cnn差不多。\n\n   它的实验使用的==word embeding 是700维==(300-700)维较好，在验证集上测试，可以试试。\n\n   ![屏幕快照 2017-03-30 下午8.49.11](https://ww3.sinaimg.cn/large/006tNbRwgy1fefi3bedcvj30ya110wol.jpg)\n\n评价：attention 比较简单，就是加了一个可训练的权重v。至于value-shared weighting，说不上效果如何，有多大提高，本文使用number of bins=600，比较细粒度的对权重进行分类学习。问答对的匹配的信号范围[-1，1]，由cosine计算得到。如下图，说不上这个value-shared weighting有什么规律？！![屏幕快照 2017-03-30 下午9.09.59](https://ww1.sinaimg.cn/large/006tNbRwgy1fefi3cvnrzj30ya110wol.jpg)\n\n## 3. Bilateral Multi-Perspective Matching for Natural Language Sentences\n\n#### 文章来源\n\n- Zhiguo Wang(王志国), Wael Hamza and Radu Florian. 2017.2.28   In eprint arXiv:1702.03814.\n- IBM T.J. Watson Research Center 1101 Kitchawan Rd, Yorktown Heights, NY 10598 {zhigwang,whamza,raduf}@us.ibm.com\n\n#### 要解决的问题和已有方法\n\n==自然语言句子匹配==是很多任务的基础，之前的工作要么是单向的，要么只应用了一种粗糙（granular,eg,word-by-word ,sentence-by-sentence）的匹配，本文在三个任务上paraphrase identiﬁcation（PI）, natural language inference(自然语言推理) and answer sentence selection（AS）都取得了state-of-art成绩。\n\n==Introduction:== NLSM(Natural language sentence matching)任务是……，在PI、推论、AS等任务中是基础。\n\nNLSM任务中已提出两种DL框架，1.“Siamese” architecture，独立处理两个输入的句子，匹配决定基于两个句子向量。 优点是，共享参数使模型更小、易训练。缺点，编码过程无交互，损失信息。2. “Matching aggregation”框架，两个句子更小粒度匹配，然后集合匹配结果。然而之前的“MG”方法有限制：只探索了word-by-word 匹配，忽略了其他粒度匹配；匹配是单向的，忽略相反方向。\n\n#### 本文的方法和创新点\n\n给定两个句子P\\Q，用biLSTM对他们分别编码，从两个方向P->Q,Q->P对其匹配。\n\n在匹配过程中，从多视野的角度，一个句子的每一步都与另一个句子的所有time-steps对应匹配。然后，另一个BiLSTM被用来集合所有匹配结果到一个固定长度的向量。再连上一个全连接层得到匹配的结果。\n\n##### 模型总览：\n\n本文提出了BiMPM(bilateral multi-perspective matching)，从下图可以看出分为5个层，\n\n![屏幕快照 2017-03-31 下午9.19.01](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3dqov0j31kw0fhdqa.jpg)\n\n其中1：第一层说词表达层由两部分组成：word embeddings and character-composed embeddings（后者是把一个词的每个字符放到LST==M里？==pre-trained with GloVe [ Pennington et al., 2014 ] or word2vec [ Mikolov et al., ] .）\n\n2. Context Representation Layer就是双向LSTM处理。\n\n3. 匹配层。本文的关键创新点，多视角的匹配操作，具体见图2.\n\n4. 第四层，结合层，就是用BiLSTM处理匹配之后的3的输出向量，这里采用最后一个时刻的向量进行连接，图中的四个绿色向量。\n\n5. 预测层，两个前向神经网络处理固定长度的匹配向量（4）的输出，最后加一个softmax层分类输出。\n\n##### 下面对于3进行详细解释：\n\n   ​\n\n   ![屏幕快照 2017-03-31 下午10.23.42](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3efja4j31kw0fhdqa.jpg)\n\n   A).  图中四种不同视角的计算匹配方法，都是用了余弦匹配$f_m$来计算，返回向量$m = f_m(v_1,v_2;W)$ ，计算两个d维向量的匹配值，$W \\in R^ {l \\times d}$ ,可训练参数。\n\n  返回向量$m=[m_1,…，m_k,…，m_l]$ ,其中$m_k = \\cos (W_k \\cdot v_1,W_K \\cdot v_2)$ ，l表示视图的维度，这里的$W_k$是W的第k行。每个P的time-step生成一个向量，即连接了8个生成的视图向量，（正反向2*4种视角）。\n\n   B). 图中四种视角只显示了单向P->Q, P中每个time-step与Q所有time-steps计算出一个m向量，总共实现双向。1）使用 Q的最后一个隐藏向量进行计算。![屏幕快照 2017-04-01 上午9.18.03](https://ww1.sinaimg.cn/large/006tNbRwgy1fefi3f1yzmj311i0vkgr5.jpg)\n\n   2）max-pooling matching: 与q的每个time-step计算fm，再求最大值。![屏幕快照 2017-04-01 上午9.21.01](https://ww3.sinaimg.cn/large/006tNbRwgy1fefi3flfngj312s12ewjh.jpg)\n\n   3）attentive-matching:计算与每个q的相关度权值，再加权求和，与这个含权值的向量计算$f_m$![屏幕快照 2017-04-01 上午9.23.24](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3g6chzj30qi06i0u2.jpg)\n\n   ![屏幕快照 2017-04-01 上午9.35.42](https://ww2.sinaimg.cn/large/006tNbRwgy1fefi3i449dj30km08ejsl.jpg)\n\n![屏幕快照 2017-04-01 上午9.36.38](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3jim9aj30m403yaav.jpg)\n\n   4)max-attentive matching,如图所示，就是在3）计算出权重值$\\alpha_{i,j}$的基础上，选出最高的cos相似值作为attentive向量，然后计算得$m_i = f_m (h^p_i,max(\\alpha_{i,j}) )for j \\in 1…N$.\n\n##### 实验的设置\n\n问题1：多视图中的l不是固定的8么？怎么设置成20？或者$l\\in\\{1,5,10,15,20\\}$实验？\n\n#### 效果和评价\n\n实验结果：在三个任务上的实验结果证明了模型的有效性。早期的工作设计手工创建的特征，只能在某一个任务或数据集上work well,很难在其他任务上普遍很好。 DL的框架，第一种，Siamese architecture，忽视了较低level的交互特征是必不可少，后期，从很多层次的粒度来匹配句子的模型越来越多，本文就属于这种框架。如下也证明了该框架。\n\n![屏幕快照 2017-04-01 上午11.34.43](https://ww2.sinaimg.cn/large/006tNbRwgy1fefi3n3nr7j31fe0qmqeb.jpg)\n\n![屏幕快照 2017-04-01 上午11.36.42](https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3of5kzj31fe0qmqeb.jpg)","slug":"skim","published":1,"updated":"2017-04-08T12:33:31.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj19a2bvs0003dcfyyq97118m","content":"<h2 id=\"1-Sequence-to-Sequence-Learning-with-Neural-Networks\"><a href=\"#1-Sequence-to-Sequence-Learning-with-Neural-Networks\" class=\"headerlink\" title=\"1. Sequence to Sequence Learning with Neural Networks\"></a>1. Sequence to Sequence Learning with Neural Networks</h2><p>DNN 不能用来map sequences to sequences.本文提出一个 一般的end-to-end方法可以去学习变长结构的序列。使用多层LSTM去匹配输入到一个固定维度向量，然后另一个deep LSTM从该中间向量解码出目标句子。</p>\n<p><img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi2t7sejj30m307kq36.jpg\" alt=\"s2sModel\"></p>\n<p><img src=\"https://ww1.sinaimg.cn/large/006tNbRwgy1fefi2z84uhj30j9020743.jpg\" alt=\"s2sEq\"></p>\n<p>LSTM的目标是估计出左边的最大概率。首先计算出固定维度表达V，表示 the input sequence $ (x_1,…,x_T)$ ,即LSTM的最后一个隐藏状态。每个右边子公式的分布是用词典里所有词的softmax表示。每个句子使用EOS结束符号，使得模型能够定义所有可能长度的句子分布。最大化log概率，训练目标：其中S是很多句子对的训练集。<img src=\"/Users/vera\\百度云同步盘\\PHD\\reread paper\\pic\\s2sloss.PNG\" alt=\"s2sloss\"></p>\n<p><strong>result :</strong> </p>\n<p><img src=\"/Users/vera/百度云同步盘\\PHD\\reread paper\\pic\\S2Sresult2.PNG\" alt=\"S2Sresult2\"></p>\n<p><strong>conclusion:</strong> 编码最大数量的短期依赖，使学习问题变得简单。反转源语言句子使结果提高很多，也是因为此。</p>\n<hr>\n<h2 id=\"2-aNMM-Ranking-Short-Answer-Texts-with-Attention-Based-Neural-Matching-Model\"><a href=\"#2-aNMM-Ranking-Short-Answer-Texts-with-Attention-Based-Neural-Matching-Model\" class=\"headerlink\" title=\"2. aNMM: Ranking Short Answer Texts with Attention-Based  Neural Matching Model\"></a>2. aNMM: Ranking Short Answer Texts with Attention-Based  Neural Matching Model</h2><h4 id=\"文章来源：\"><a href=\"#文章来源：\" class=\"headerlink\" title=\"文章来源：\"></a>文章来源：</h4><p><strong>CIKM’16</strong> , October 24-28, 2016, Indianapolis, IN, USA</p>\n<p>==Liu Yan==g 1  Qingyao Ai 1 Jiafeng Guo 2 W. Bruce Croft 1</p>\n<ol>\n<li>Center for Intelligent Information Retrieval, University of Massachusetts Amherst, MA, USA</li>\n</ol>\n<ol>\n<li>CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China</li>\n</ol>\n<p>{lyang, aiqy, croft}@cs.umass.edu, guojiafeng@ict.ac.cn</p>\n<p>题外话：本文的实现是基于java的，难怪最后模型是softmax那样的训练方法。<a href=\"https://github.com/yangliuy/aNMM-CIKM16\" target=\"_blank\" rel=\"external\">https://github.com/yangliuy/aNMM-CIKM16</a> 开源程序。</p>\n<h4 id=\"要解决的问题和已有方法：\"><a href=\"#要解决的问题和已有方法：\" class=\"headerlink\" title=\"要解决的问题和已有方法：\"></a>要解决的问题和已有方法：</h4><p>基于attention的神经匹配模型去排序短文本；数据集TREC QA。</p>\n<p>之前模型，结合额外特征，如word overlap&amp; BM25;没有这些特征，模型表现差于语言特征工程。</p>\n<p>本文解决问题：RQ1：不结合额外特征，DL模型能实现或者超过使用特征工程的方法么？</p>\n<p>RQ2：结合额外特征，本文模型能优于当前QA的最好模型么？</p>\n<p>之前不足：</p>\n<ol>\n<li>之前模型的不止为QA匹配：CNN使用position-valued权重，LSTM问答对序列，没有直接交互不能获得有效匹配信号。</li>\n<li>缺少建模问题的关注点。</li>\n</ol>\n<h4 id=\"本文方法和创新点\"><a href=\"#本文方法和创新点\" class=\"headerlink\" title=\"本文方法和创新点\"></a>本文方法和创新点</h4><ol>\n<li><p>采用value-shared weighting scheme 而不是 position-shared weighting来结合匹配信号，</p>\n</li>\n<li><p>并且包含问题词重要性学习，通过问题attention 网络。</p>\n</li>\n<li><p>拓展的实验评估和不错的结果。</p>\n<p><img src=\"https://ww2.sinaimg.cn/large/006tNbRwgy1fefi32hfmgj31kw0w4wxi.jpg\" alt=\"model2017-03-30 下午4.46.32\"></p>\n<p>从输入层到隐藏层的计算：valued-shared weighting，如何体现的？是否有意义？</p>\n<p><img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi33cmi4j30f403wmxa.jpg\" alt=\"eq2017-03-30 下午4.48.23\"></p>\n<p>从隐藏层到输出层（还有一个softmax 门学习question权重）：question attention network,通过这个attention网络的输出 来 给定“  问答对匹配信号”的 权重  即这里的v，这里使用了nomalize，归一化。</p>\n<p><img src=\"https://ww3.sinaimg.cn/large/006tNbRwgy1fefi34mgmtj30to050t9b.jpg\" alt=\"屏幕快照 2017-03-30 下午5.00.52\"></p>\n<p>aNMM-2增加了$r_t$  T为隐藏向量节点数，图中多个黄色点。w为多个权重向量，二维矩阵。两个隐藏层，相比于上个公式，1）增加线性结合的结点输出，2）额外激活函数$\\tau$ 。<img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi39bmd3j30p80583yz.jpg\" alt=\"屏幕快照 2017-03-30 下午5.57.32\"></p>\n<p>推导反向传播的公式，损失函数含义？</p>\n<p>​</p>\n<h4 id=\"效果和评价\"><a href=\"#效果和评价\" class=\"headerlink\" title=\"效果和评价\"></a>效果和评价</h4><p>可以看出ANMM1模型效果其实更好，而且加入了attention提高不多。和我biLSTM/cnn差不多。</p>\n<p>它的实验使用的==word embeding 是700维==(300-700)维较好，在验证集上测试，可以试试。</p>\n<p><img src=\"https://ww3.sinaimg.cn/large/006tNbRwgy1fefi3bedcvj30ya110wol.jpg\" alt=\"屏幕快照 2017-03-30 下午8.49.11\"></p>\n</li>\n</ol>\n<p>评价：attention 比较简单，就是加了一个可训练的权重v。至于value-shared weighting，说不上效果如何，有多大提高，本文使用number of bins=600，比较细粒度的对权重进行分类学习。问答对的匹配的信号范围[-1，1]，由cosine计算得到。如下图，说不上这个value-shared weighting有什么规律？！<img src=\"https://ww1.sinaimg.cn/large/006tNbRwgy1fefi3cvnrzj30ya110wol.jpg\" alt=\"屏幕快照 2017-03-30 下午9.09.59\"></p>\n<h2 id=\"3-Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences\"><a href=\"#3-Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences\" class=\"headerlink\" title=\"3. Bilateral Multi-Perspective Matching for Natural Language Sentences\"></a>3. Bilateral Multi-Perspective Matching for Natural Language Sentences</h2><h4 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h4><ul>\n<li>Zhiguo Wang(王志国), Wael Hamza and Radu Florian. 2017.2.28   In eprint arXiv:1702.03814.</li>\n<li>IBM T.J. Watson Research Center 1101 Kitchawan Rd, Yorktown Heights, NY 10598 {zhigwang,whamza,raduf}@us.ibm.com</li>\n</ul>\n<h4 id=\"要解决的问题和已有方法\"><a href=\"#要解决的问题和已有方法\" class=\"headerlink\" title=\"要解决的问题和已有方法\"></a>要解决的问题和已有方法</h4><p>==自然语言句子匹配==是很多任务的基础，之前的工作要么是单向的，要么只应用了一种粗糙（granular,eg,word-by-word ,sentence-by-sentence）的匹配，本文在三个任务上paraphrase identiﬁcation（PI）, natural language inference(自然语言推理) and answer sentence selection（AS）都取得了state-of-art成绩。</p>\n<p>==Introduction:== NLSM(Natural language sentence matching)任务是……，在PI、推论、AS等任务中是基础。</p>\n<p>NLSM任务中已提出两种DL框架，1.“Siamese” architecture，独立处理两个输入的句子，匹配决定基于两个句子向量。 优点是，共享参数使模型更小、易训练。缺点，编码过程无交互，损失信息。2. “Matching aggregation”框架，两个句子更小粒度匹配，然后集合匹配结果。然而之前的“MG”方法有限制：只探索了word-by-word 匹配，忽略了其他粒度匹配；匹配是单向的，忽略相反方向。</p>\n<h4 id=\"本文的方法和创新点\"><a href=\"#本文的方法和创新点\" class=\"headerlink\" title=\"本文的方法和创新点\"></a>本文的方法和创新点</h4><p>给定两个句子P\\Q，用biLSTM对他们分别编码，从两个方向P-&gt;Q,Q-&gt;P对其匹配。</p>\n<p>在匹配过程中，从多视野的角度，一个句子的每一步都与另一个句子的所有time-steps对应匹配。然后，另一个BiLSTM被用来集合所有匹配结果到一个固定长度的向量。再连上一个全连接层得到匹配的结果。</p>\n<h5 id=\"模型总览：\"><a href=\"#模型总览：\" class=\"headerlink\" title=\"模型总览：\"></a>模型总览：</h5><p>本文提出了BiMPM(bilateral multi-perspective matching)，从下图可以看出分为5个层，</p>\n<p><img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3dqov0j31kw0fhdqa.jpg\" alt=\"屏幕快照 2017-03-31 下午9.19.01\"></p>\n<p>其中1：第一层说词表达层由两部分组成：word embeddings and character-composed embeddings（后者是把一个词的每个字符放到LST==M里？==pre-trained with GloVe [ Pennington et al., 2014 ] or word2vec [ Mikolov et al., ] .）</p>\n<ol>\n<li><p>Context Representation Layer就是双向LSTM处理。</p>\n</li>\n<li><p>匹配层。本文的关键创新点，多视角的匹配操作，具体见图2.</p>\n</li>\n<li><p>第四层，结合层，就是用BiLSTM处理匹配之后的3的输出向量，这里采用最后一个时刻的向量进行连接，图中的四个绿色向量。</p>\n</li>\n<li><p>预测层，两个前向神经网络处理固定长度的匹配向量（4）的输出，最后加一个softmax层分类输出。</p>\n</li>\n</ol>\n<h5 id=\"下面对于3进行详细解释：\"><a href=\"#下面对于3进行详细解释：\" class=\"headerlink\" title=\"下面对于3进行详细解释：\"></a>下面对于3进行详细解释：</h5><p>   ​</p>\n<p>   <img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3efja4j31kw0fhdqa.jpg\" alt=\"屏幕快照 2017-03-31 下午10.23.42\"></p>\n<p>   A).  图中四种不同视角的计算匹配方法，都是用了余弦匹配$f_m$来计算，返回向量$m = f_m(v_1,v_2;W)$ ，计算两个d维向量的匹配值，$W \\in R^ {l \\times d}$ ,可训练参数。</p>\n<p>  返回向量$m=[m_1,…，m_k,…，m_l]$ ,其中$m_k = \\cos (W_k \\cdot v_1,W_K \\cdot v_2)$ ，l表示视图的维度，这里的$W_k$是W的第k行。每个P的time-step生成一个向量，即连接了8个生成的视图向量，（正反向2*4种视角）。</p>\n<p>   B). 图中四种视角只显示了单向P-&gt;Q, P中每个time-step与Q所有time-steps计算出一个m向量，总共实现双向。1）使用 Q的最后一个隐藏向量进行计算。<img src=\"https://ww1.sinaimg.cn/large/006tNbRwgy1fefi3f1yzmj311i0vkgr5.jpg\" alt=\"屏幕快照 2017-04-01 上午9.18.03\"></p>\n<p>   2）max-pooling matching: 与q的每个time-step计算fm，再求最大值。<img src=\"https://ww3.sinaimg.cn/large/006tNbRwgy1fefi3flfngj312s12ewjh.jpg\" alt=\"屏幕快照 2017-04-01 上午9.21.01\"></p>\n<p>   3）attentive-matching:计算与每个q的相关度权值，再加权求和，与这个含权值的向量计算$f_m$<img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3g6chzj30qi06i0u2.jpg\" alt=\"屏幕快照 2017-04-01 上午9.23.24\"></p>\n<p>   <img src=\"https://ww2.sinaimg.cn/large/006tNbRwgy1fefi3i449dj30km08ejsl.jpg\" alt=\"屏幕快照 2017-04-01 上午9.35.42\"></p>\n<p><img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3jim9aj30m403yaav.jpg\" alt=\"屏幕快照 2017-04-01 上午9.36.38\"></p>\n<p>   4)max-attentive matching,如图所示，就是在3）计算出权重值$\\alpha_{i,j}$的基础上，选出最高的cos相似值作为attentive向量，然后计算得$m_i = f_m (h^p<em>i,max(\\alpha</em>{i,j}) )for j \\in 1…N$.</p>\n<h5 id=\"实验的设置\"><a href=\"#实验的设置\" class=\"headerlink\" title=\"实验的设置\"></a>实验的设置</h5><p>问题1：多视图中的l不是固定的8么？怎么设置成20？或者$l\\in{1,5,10,15,20}$实验？</p>\n<h4 id=\"效果和评价-1\"><a href=\"#效果和评价-1\" class=\"headerlink\" title=\"效果和评价\"></a>效果和评价</h4><p>实验结果：在三个任务上的实验结果证明了模型的有效性。早期的工作设计手工创建的特征，只能在某一个任务或数据集上work well,很难在其他任务上普遍很好。 DL的框架，第一种，Siamese architecture，忽视了较低level的交互特征是必不可少，后期，从很多层次的粒度来匹配句子的模型越来越多，本文就属于这种框架。如下也证明了该框架。</p>\n<p><img src=\"https://ww2.sinaimg.cn/large/006tNbRwgy1fefi3n3nr7j31fe0qmqeb.jpg\" alt=\"屏幕快照 2017-04-01 上午11.34.43\"></p>\n<p><img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3of5kzj31fe0qmqeb.jpg\" alt=\"屏幕快照 2017-04-01 上午11.36.42\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1-Sequence-to-Sequence-Learning-with-Neural-Networks\"><a href=\"#1-Sequence-to-Sequence-Learning-with-Neural-Networks\" class=\"headerlink\" title=\"1. Sequence to Sequence Learning with Neural Networks\"></a>1. Sequence to Sequence Learning with Neural Networks</h2><p>DNN 不能用来map sequences to sequences.本文提出一个 一般的end-to-end方法可以去学习变长结构的序列。使用多层LSTM去匹配输入到一个固定维度向量，然后另一个deep LSTM从该中间向量解码出目标句子。</p>\n<p><img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi2t7sejj30m307kq36.jpg\" alt=\"s2sModel\"></p>\n<p><img src=\"https://ww1.sinaimg.cn/large/006tNbRwgy1fefi2z84uhj30j9020743.jpg\" alt=\"s2sEq\"></p>\n<p>LSTM的目标是估计出左边的最大概率。首先计算出固定维度表达V，表示 the input sequence $ (x_1,…,x_T)$ ,即LSTM的最后一个隐藏状态。每个右边子公式的分布是用词典里所有词的softmax表示。每个句子使用EOS结束符号，使得模型能够定义所有可能长度的句子分布。最大化log概率，训练目标：其中S是很多句子对的训练集。<img src=\"/Users/vera\\百度云同步盘\\PHD\\reread paper\\pic\\s2sloss.PNG\" alt=\"s2sloss\"></p>\n<p><strong>result :</strong> </p>\n<p><img src=\"/Users/vera/百度云同步盘\\PHD\\reread paper\\pic\\S2Sresult2.PNG\" alt=\"S2Sresult2\"></p>\n<p><strong>conclusion:</strong> 编码最大数量的短期依赖，使学习问题变得简单。反转源语言句子使结果提高很多，也是因为此。</p>\n<hr>\n<h2 id=\"2-aNMM-Ranking-Short-Answer-Texts-with-Attention-Based-Neural-Matching-Model\"><a href=\"#2-aNMM-Ranking-Short-Answer-Texts-with-Attention-Based-Neural-Matching-Model\" class=\"headerlink\" title=\"2. aNMM: Ranking Short Answer Texts with Attention-Based  Neural Matching Model\"></a>2. aNMM: Ranking Short Answer Texts with Attention-Based  Neural Matching Model</h2><h4 id=\"文章来源：\"><a href=\"#文章来源：\" class=\"headerlink\" title=\"文章来源：\"></a>文章来源：</h4><p><strong>CIKM’16</strong> , October 24-28, 2016, Indianapolis, IN, USA</p>\n<p>==Liu Yan==g 1  Qingyao Ai 1 Jiafeng Guo 2 W. Bruce Croft 1</p>\n<ol>\n<li>Center for Intelligent Information Retrieval, University of Massachusetts Amherst, MA, USA</li>\n</ol>\n<ol>\n<li>CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China</li>\n</ol>\n<p>{lyang, aiqy, croft}@cs.umass.edu, guojiafeng@ict.ac.cn</p>\n<p>题外话：本文的实现是基于java的，难怪最后模型是softmax那样的训练方法。<a href=\"https://github.com/yangliuy/aNMM-CIKM16\">https://github.com/yangliuy/aNMM-CIKM16</a> 开源程序。</p>\n<h4 id=\"要解决的问题和已有方法：\"><a href=\"#要解决的问题和已有方法：\" class=\"headerlink\" title=\"要解决的问题和已有方法：\"></a>要解决的问题和已有方法：</h4><p>基于attention的神经匹配模型去排序短文本；数据集TREC QA。</p>\n<p>之前模型，结合额外特征，如word overlap&amp; BM25;没有这些特征，模型表现差于语言特征工程。</p>\n<p>本文解决问题：RQ1：不结合额外特征，DL模型能实现或者超过使用特征工程的方法么？</p>\n<p>RQ2：结合额外特征，本文模型能优于当前QA的最好模型么？</p>\n<p>之前不足：</p>\n<ol>\n<li>之前模型的不止为QA匹配：CNN使用position-valued权重，LSTM问答对序列，没有直接交互不能获得有效匹配信号。</li>\n<li>缺少建模问题的关注点。</li>\n</ol>\n<h4 id=\"本文方法和创新点\"><a href=\"#本文方法和创新点\" class=\"headerlink\" title=\"本文方法和创新点\"></a>本文方法和创新点</h4><ol>\n<li><p>采用value-shared weighting scheme 而不是 position-shared weighting来结合匹配信号，</p>\n</li>\n<li><p>并且包含问题词重要性学习，通过问题attention 网络。</p>\n</li>\n<li><p>拓展的实验评估和不错的结果。</p>\n<p><img src=\"https://ww2.sinaimg.cn/large/006tNbRwgy1fefi32hfmgj31kw0w4wxi.jpg\" alt=\"model2017-03-30 下午4.46.32\"></p>\n<p>从输入层到隐藏层的计算：valued-shared weighting，如何体现的？是否有意义？</p>\n<p><img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi33cmi4j30f403wmxa.jpg\" alt=\"eq2017-03-30 下午4.48.23\"></p>\n<p>从隐藏层到输出层（还有一个softmax 门学习question权重）：question attention network,通过这个attention网络的输出 来 给定“  问答对匹配信号”的 权重  即这里的v，这里使用了nomalize，归一化。</p>\n<p><img src=\"https://ww3.sinaimg.cn/large/006tNbRwgy1fefi34mgmtj30to050t9b.jpg\" alt=\"屏幕快照 2017-03-30 下午5.00.52\"></p>\n<p>aNMM-2增加了$r_t$  T为隐藏向量节点数，图中多个黄色点。w为多个权重向量，二维矩阵。两个隐藏层，相比于上个公式，1）增加线性结合的结点输出，2）额外激活函数$\\tau$ 。<img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi39bmd3j30p80583yz.jpg\" alt=\"屏幕快照 2017-03-30 下午5.57.32\"></p>\n<p>推导反向传播的公式，损失函数含义？</p>\n<p>​</p>\n<h4 id=\"效果和评价\"><a href=\"#效果和评价\" class=\"headerlink\" title=\"效果和评价\"></a>效果和评价</h4><p>可以看出ANMM1模型效果其实更好，而且加入了attention提高不多。和我biLSTM/cnn差不多。</p>\n<p>它的实验使用的==word embeding 是700维==(300-700)维较好，在验证集上测试，可以试试。</p>\n<p><img src=\"https://ww3.sinaimg.cn/large/006tNbRwgy1fefi3bedcvj30ya110wol.jpg\" alt=\"屏幕快照 2017-03-30 下午8.49.11\"></p>\n</li>\n</ol>\n<p>评价：attention 比较简单，就是加了一个可训练的权重v。至于value-shared weighting，说不上效果如何，有多大提高，本文使用number of bins=600，比较细粒度的对权重进行分类学习。问答对的匹配的信号范围[-1，1]，由cosine计算得到。如下图，说不上这个value-shared weighting有什么规律？！<img src=\"https://ww1.sinaimg.cn/large/006tNbRwgy1fefi3cvnrzj30ya110wol.jpg\" alt=\"屏幕快照 2017-03-30 下午9.09.59\"></p>\n<h2 id=\"3-Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences\"><a href=\"#3-Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences\" class=\"headerlink\" title=\"3. Bilateral Multi-Perspective Matching for Natural Language Sentences\"></a>3. Bilateral Multi-Perspective Matching for Natural Language Sentences</h2><h4 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h4><ul>\n<li>Zhiguo Wang(王志国), Wael Hamza and Radu Florian. 2017.2.28   In eprint arXiv:1702.03814.</li>\n<li>IBM T.J. Watson Research Center 1101 Kitchawan Rd, Yorktown Heights, NY 10598 {zhigwang,whamza,raduf}@us.ibm.com</li>\n</ul>\n<h4 id=\"要解决的问题和已有方法\"><a href=\"#要解决的问题和已有方法\" class=\"headerlink\" title=\"要解决的问题和已有方法\"></a>要解决的问题和已有方法</h4><p>==自然语言句子匹配==是很多任务的基础，之前的工作要么是单向的，要么只应用了一种粗糙（granular,eg,word-by-word ,sentence-by-sentence）的匹配，本文在三个任务上paraphrase identiﬁcation（PI）, natural language inference(自然语言推理) and answer sentence selection（AS）都取得了state-of-art成绩。</p>\n<p>==Introduction:== NLSM(Natural language sentence matching)任务是……，在PI、推论、AS等任务中是基础。</p>\n<p>NLSM任务中已提出两种DL框架，1.“Siamese” architecture，独立处理两个输入的句子，匹配决定基于两个句子向量。 优点是，共享参数使模型更小、易训练。缺点，编码过程无交互，损失信息。2. “Matching aggregation”框架，两个句子更小粒度匹配，然后集合匹配结果。然而之前的“MG”方法有限制：只探索了word-by-word 匹配，忽略了其他粒度匹配；匹配是单向的，忽略相反方向。</p>\n<h4 id=\"本文的方法和创新点\"><a href=\"#本文的方法和创新点\" class=\"headerlink\" title=\"本文的方法和创新点\"></a>本文的方法和创新点</h4><p>给定两个句子P\\Q，用biLSTM对他们分别编码，从两个方向P-&gt;Q,Q-&gt;P对其匹配。</p>\n<p>在匹配过程中，从多视野的角度，一个句子的每一步都与另一个句子的所有time-steps对应匹配。然后，另一个BiLSTM被用来集合所有匹配结果到一个固定长度的向量。再连上一个全连接层得到匹配的结果。</p>\n<h5 id=\"模型总览：\"><a href=\"#模型总览：\" class=\"headerlink\" title=\"模型总览：\"></a>模型总览：</h5><p>本文提出了BiMPM(bilateral multi-perspective matching)，从下图可以看出分为5个层，</p>\n<p><img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3dqov0j31kw0fhdqa.jpg\" alt=\"屏幕快照 2017-03-31 下午9.19.01\"></p>\n<p>其中1：第一层说词表达层由两部分组成：word embeddings and character-composed embeddings（后者是把一个词的每个字符放到LST==M里？==pre-trained with GloVe [ Pennington et al., 2014 ] or word2vec [ Mikolov et al., ] .）</p>\n<ol>\n<li><p>Context Representation Layer就是双向LSTM处理。</p>\n</li>\n<li><p>匹配层。本文的关键创新点，多视角的匹配操作，具体见图2.</p>\n</li>\n<li><p>第四层，结合层，就是用BiLSTM处理匹配之后的3的输出向量，这里采用最后一个时刻的向量进行连接，图中的四个绿色向量。</p>\n</li>\n<li><p>预测层，两个前向神经网络处理固定长度的匹配向量（4）的输出，最后加一个softmax层分类输出。</p>\n</li>\n</ol>\n<h5 id=\"下面对于3进行详细解释：\"><a href=\"#下面对于3进行详细解释：\" class=\"headerlink\" title=\"下面对于3进行详细解释：\"></a>下面对于3进行详细解释：</h5><p>   ​</p>\n<p>   <img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3efja4j31kw0fhdqa.jpg\" alt=\"屏幕快照 2017-03-31 下午10.23.42\"></p>\n<p>   A).  图中四种不同视角的计算匹配方法，都是用了余弦匹配$f_m$来计算，返回向量$m = f_m(v_1,v_2;W)$ ，计算两个d维向量的匹配值，$W \\in R^ {l \\times d}$ ,可训练参数。</p>\n<p>  返回向量$m=[m_1,…，m_k,…，m_l]$ ,其中$m_k = \\cos (W_k \\cdot v_1,W_K \\cdot v_2)$ ，l表示视图的维度，这里的$W_k$是W的第k行。每个P的time-step生成一个向量，即连接了8个生成的视图向量，（正反向2*4种视角）。</p>\n<p>   B). 图中四种视角只显示了单向P-&gt;Q, P中每个time-step与Q所有time-steps计算出一个m向量，总共实现双向。1）使用 Q的最后一个隐藏向量进行计算。<img src=\"https://ww1.sinaimg.cn/large/006tNbRwgy1fefi3f1yzmj311i0vkgr5.jpg\" alt=\"屏幕快照 2017-04-01 上午9.18.03\"></p>\n<p>   2）max-pooling matching: 与q的每个time-step计算fm，再求最大值。<img src=\"https://ww3.sinaimg.cn/large/006tNbRwgy1fefi3flfngj312s12ewjh.jpg\" alt=\"屏幕快照 2017-04-01 上午9.21.01\"></p>\n<p>   3）attentive-matching:计算与每个q的相关度权值，再加权求和，与这个含权值的向量计算$f_m$<img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3g6chzj30qi06i0u2.jpg\" alt=\"屏幕快照 2017-04-01 上午9.23.24\"></p>\n<p>   <img src=\"https://ww2.sinaimg.cn/large/006tNbRwgy1fefi3i449dj30km08ejsl.jpg\" alt=\"屏幕快照 2017-04-01 上午9.35.42\"></p>\n<p><img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3jim9aj30m403yaav.jpg\" alt=\"屏幕快照 2017-04-01 上午9.36.38\"></p>\n<p>   4)max-attentive matching,如图所示，就是在3）计算出权重值$\\alpha_{i,j}$的基础上，选出最高的cos相似值作为attentive向量，然后计算得$m_i = f_m (h^p<em>i,max(\\alpha</em>{i,j}) )for j \\in 1…N$.</p>\n<h5 id=\"实验的设置\"><a href=\"#实验的设置\" class=\"headerlink\" title=\"实验的设置\"></a>实验的设置</h5><p>问题1：多视图中的l不是固定的8么？怎么设置成20？或者$l\\in{1,5,10,15,20}$实验？</p>\n<h4 id=\"效果和评价-1\"><a href=\"#效果和评价-1\" class=\"headerlink\" title=\"效果和评价\"></a>效果和评价</h4><p>实验结果：在三个任务上的实验结果证明了模型的有效性。早期的工作设计手工创建的特征，只能在某一个任务或数据集上work well,很难在其他任务上普遍很好。 DL的框架，第一种，Siamese architecture，忽视了较低level的交互特征是必不可少，后期，从很多层次的粒度来匹配句子的模型越来越多，本文就属于这种框架。如下也证明了该框架。</p>\n<p><img src=\"https://ww2.sinaimg.cn/large/006tNbRwgy1fefi3n3nr7j31fe0qmqeb.jpg\" alt=\"屏幕快照 2017-04-01 上午11.34.43\"></p>\n<p><img src=\"https://ww4.sinaimg.cn/large/006tNbRwgy1fefi3of5kzj31fe0qmqeb.jpg\" alt=\"屏幕快照 2017-04-01 上午11.36.42\"></p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cj19a2bvs0003dcfyyq97118m","tag_id":"cj19a2bvt0004dcfyvtz93kho","_id":"cj19a2bvx0005dcfyfu8k0wko"}],"Tag":[{"name":"自然语言处理","_id":"cj19a2bvt0004dcfyvtz93kho"}]}}